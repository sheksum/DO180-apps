cat <<EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: image-registry-storage
  namespace: openshift-image-registry
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Gi
  storageClassName: netapp-infra
EOF

oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"managementState":"Managed","storage":{"pvc":{"claim":"image-registry-storage"}},"replicas":2}}'



oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"defaultRoute":true}}'


oc get route -n openshift-image-registry


HOST=$(oc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}')
podman login -u kubeadmin -p $(oc whoami -t) $HOST
podman pull busybox
podman tag busybox $HOST/default/busybox:test
podman push $HOST/default/busybox:test


hsuma@plnx-admin:~$ podman login -u kubeadmin -p $(oc whoami -t) $HOST --tls-verify=false
error: no token is currently in use for this session
ERRO[0000] cannot find UID/GID for user hsuma: No subuid ranges found for user "hsuma" in /etc/subuid - check rootless mode in man pages.
WARN[0000] using rootless single mapping into the namespace. This might break some images. Check /etc/subuid and /etc/subgid for adding sub*ids
Error: no registries found in registries.conf, a registry must be provided
hsuma@plnx-admin:~$


oc rsh -n openshift-image-registry $(oc get pods -n openshift-image-registry -l docker-registry=default -o jsonpath='{.items[0].metadata.name}') touch /registry/test-write && echo "Write OK"



hsuma@plnx-admin:~$ oc rsh -n openshift-image-registry $(oc get pods -n openshift-image-registry -l docker-registry=default -o jsonpath='{.items[0].metadata.name}') touch /registry/test-write && echo "Write OK"
^Ccommand terminated with exit code 130
hsuma@plnx-admin:~$
hsuma@plnx-admin:~$
hsuma@plnx-admin:~$ oc describe pvc image-registry-storage -n openshift-image-registry
Name:          image-registry-storage
Namespace:     openshift-image-registry
StorageClass:  netapp-infra
Status:        Bound
Volume:        pvc-766e03dc-d5e4-496f-9bd0-8b38905999dd
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-provisioner: csi.trident.netapp.io
               volume.kubernetes.io/storage-provisioner: csi.trident.netapp.io
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      100Gi
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       image-registry-5b49fc8797-7m7d4
               image-registry-5b49fc8797-mqmsp
Events:
  Type    Reason                 Age                From                                                                                            Message
  ----    ------                 ----               ----                                                                                            -------
  Normal  Provisioning           21m                csi.trident.netapp.io_trident-controller-7cb57b6f6d-vgf24_3b4a98db-5441-47a5-abfc-6e7ade299819  External provisioner is provisioning volume for claim "openshift-image-registry/image-registry-storage"
  Normal  ExternalProvisioning   21m (x2 over 21m)  persistentvolume-controller                                                                     Waiting for a volume to be created either by the external provisioner 'csi.trident.netapp.io' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
  Normal  ProvisioningSuccess    21m                csi.trident.netapp.io                                                                           provisioned a volume
  Normal  ProvisioningSucceeded  21m                csi.trident.netapp.io_trident-controller-7cb57b6f6d-vgf24_3b4a98db-5441-47a5-abfc-6e7ade299819  Successfully provisioned volume pvc-766e03dc-d5e4-496f-9bd0-8b38905999dd
hsuma@plnx-admin:~$


plano_aff_cl::> volume show -vserver ocp_infra_svm
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
ocp_infra_svm
          ocp_infra_svm_root
                       aggr1_n5     online     RW          1GB    970.9MB    0%

plano_aff_cl::>
plano_aff_cl::>
plano_aff_cl::> volume show -vserver ocp_app_svm
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
ocp_app_svm
          ocp_app_svm_root
                       aggr1_n6     online     RW          1GB    970.8MB    0%
ocp_app_svm
          trident_pvc_131edc3f_480b_47dd_bc98_b077bd05b17b
                       aggr1_n4     online     RW       1.05GB     1023MB    0%
ocp_app_svm
          trident_pvc_766e03dc_d5e4_496f_9bd0_8b38905999dd
                       aggr1_n5     online     RW      105.3GB   100.00GB    0%
3 entries were displayed.

plano_aff_cl::>


plano_aff_cl::> export-policy rule show -vserver ocp_app_svm -fields rorule,rwrule,superuser
vserver     policyname         ruleindex rorule rwrule superuser
----------- ------------------ --------- ------ ------ ---------
ocp_app_svm backend_app_access 1         any    any    any
ocp_app_svm backend_app_access 2         any    any    any
ocp_app_svm backend_app_access 3         any    any    any
ocp_app_svm backend_app_access 4         any    any    any
ocp_app_svm default            1         any    any    any
5 entries were displayed.

plano_aff_cl::>


plano_aff_cl::> export-policy rule show -vserver ocp_infra_svm
             Policy          Rule    Access   Client                RO
Vserver      Name            Index   Protocol Match                 Rule
------------ --------------- ------  -------- --------------------- ---------
ocp_infra_svm
             backend_infra_access
                             1       nfs3,    10.172.192.0/24       any
                                     nfs4,
                                     nfs
ocp_infra_svm
             backend_infra_access
                             2       nfs3,    10.172.196.0/24       any
                                     nfs4,
                                     nfs
ocp_infra_svm
             backend_infra_access
                             3       nfs3,    10.172.200.0/24       any
                                     nfs4,
                                     nfs
ocp_infra_svm
             backend_infra_access
                             4       nfs3,    10.172.248.0/24       any
                                     nfs4,
                                     nfs
ocp_infra_svm
             default         2       nfs3,    0.0.0.0/0             any
                                     nfs4,
                                     nfs
5 entries were displayed.

plano_aff_cl::> export-policy rule show -vserver ocp_app_svm
             Policy          Rule    Access   Client                RO
Vserver      Name            Index   Protocol Match                 Rule
------------ --------------- ------  -------- --------------------- ---------
ocp_app_svm  backend_app_access
                             1       nfs3,    10.172.194.0/24       any
                                     nfs4,
                                     nfs
ocp_app_svm  backend_app_access
                             2       nfs3,    10.172.198.0/24       any
                                     nfs4,
                                     nfs
ocp_app_svm  backend_app_access
                             3       nfs3,    10.172.202.0/24       any
                                     nfs4,
                                     nfs
ocp_app_svm  backend_app_access
                             4       nfs3,    10.172.248.0/24       any
                                     nfs4,
                                     nfs
ocp_app_svm  default         1       nfs3,    0.0.0.0/0             any
                                     nfs4,
                                     nfs
5 entries were displayed.

plano_aff_cl::>



 plano_aff_cl::> export-policy rule show -vserver ocp_app_svm -fields rorule,rwrule,superuser
vserver     policyname         ruleindex rorule rwrule superuser
----------- ------------------ --------- ------ ------ ---------
ocp_app_svm backend_app_access 1         any    any    any
ocp_app_svm backend_app_access 2         any    any    any
ocp_app_svm backend_app_access 3         any    any    any
ocp_app_svm backend_app_access 4         any    any    any
ocp_app_svm default            1         any    any    any
5 entries were displayed.

plano_aff_cl::>


[core@ocp-ai-master02 ~]$ mount | grep trident
10.172.194.22:/trident_pvc_766e03dc_d5e4_496f_9bd0_8b38905999dd on /var/lib/kubelet/pods/b1e78fdd-8266-4bd0-af4a-ae362b44a2e0/volumes/kubernetes.io~csi/pvc-766e03dc-d5e4-496f-9bd0-8b38905999dd/mount type nfs4 (rw,relatime,vers=4.1,rsize=65536,wsize=65536,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.172.194.151,local_lock=none,addr=10.172.194.22)
[core@ocp-ai-master02 ~]$


oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"nodeSelector":{"node-role.kubernetes.io/master":""}}}'



plano_aff_cl::> network interface show -vserver ocp_infra_svm
            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
ocp_infra_svm
            ocp_infra_dev_lif01
                         up/up    10.172.192.22/23   plano_aff_cl-05
                                                                   a0a-1792
                                                                           true
            ocp_infra_dev_lif02
                         up/up    10.172.192.23/23   plano_aff_cl-06
                                                                   a0a-1792
                                                                           true
            ocp_infra_mgmt
                         up/up    10.172.254.46/24   plano_aff_cl-05
                                                                   a0a-1570
                                                                           true
            ocp_infra_prod_lif01
                         up/up    10.172.200.22/23   plano_aff_cl-05
                                                                   a0a-1793
                                                                           true
            ocp_infra_prod_lif02
                         up/up    10.172.200.23/23   plano_aff_cl-06
                                                                   a0a-1793
                                                                           true
            ocp_infra_test_lif01
                         up/up    10.172.196.22/23   plano_aff_cl-05
                                                                   a0a-1796
                                                                           true
            ocp_infra_test_lif02
                         up/up    10.172.196.23/23   plano_aff_cl-06
                                                                   a0a-1796
                                                                           true
7 entries were displayed.

plano_aff_cl::> network interface show -vserver ocp_app_svm
            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
ocp_app_svm
            ocp_app_dev_lif01
                         up/up    10.172.194.22/23   plano_aff_cl-05
                                                                   a0a-1794
                                                                           true
            ocp_app_dev_lif02
                         up/up    10.172.194.23/23   plano_aff_cl-06
                                                                   a0a-1794
                                                                           true
            ocp_app_mgmt up/up    10.172.254.47/24   plano_aff_cl-06
                                                                   a0a-1570
                                                                           true
            ocp_app_prod_lif01
                         up/up    10.172.202.22/23   plano_aff_cl-05
                                                                   a0a-1797
                                                                           true
            ocp_app_prod_lif02
                         up/up    10.172.202.23/23   plano_aff_cl-06
                                                                   a0a-1797
                                                                           true
            ocp_app_test_lif01
                         up/up    10.172.198.22/23   plano_aff_cl-05
                                                                   a0a-1798
                                                                           true
            ocp_app_test_lif02
                         up/up    10.172.198.23/23   plano_aff_cl-06
                                                                   a0a-1798
                                                                           true
7 entries were displayed.

plano_aff_cl::>


lano_aff_cl::> security login show -vserver ocp_infra_svm

Vserver: ocp_infra_svm
                                                                 Second
User/Group                 Authentication                 Acct   Authentication
Name           Application Method        Role Name        Locked Method
-------------- ----------- ------------- ---------------- ------ --------------
vsadmin        http        password      vsadmin          yes    none
vsadmin        ontapi      password      vsadmin          yes    none
vsadmin        ssh         password      vsadmin          yes    none
3 entries were displayed.

plano_aff_cl::>
plano_aff_cl::> security login show -vserver ocp_app_svm

Vserver: ocp_app_svm
                                                                 Second
User/Group                 Authentication                 Acct   Authentication
Name           Application Method        Role Name        Locked Method
-------------- ----------- ------------- ---------------- ------ --------------
vsadmin        http        password      vsadmin          yes    none
vsadmin        ontapi      password      vsadmin          yes    none
vsadmin        ssh         password      vsadmin          yes    none
3 entries were displayed.

plano_aff_cl::>




oc -n trident create secret generic backend-infra-secret --from-literal=username=vsadmin --from-literal=password='<infra-password>' --dry-run=client -o yaml | oc apply -f -

oc -n trident create secret generic backend-app-secret --from-literal=username=vsadmin --from-literal=password='<app-password>' --dry-run=client -o yaml | oc apply -f -

oc -n trident patch tbc backend-infra-nfs --type merge --patch '{"spec":{"managementLIF":"10.172.254.46"}}'

oc -n trident patch tbc backend-app-nfs --type merge --patch '{"spec":{"managementLIF":"10.172.254.47"}}'

oc get tbc -n trident

hsuma@plnx-admin:~$
hsuma@plnx-admin:~$ oc -n trident create secret generic backend-infra-secret --from-literal=username=vsadmin --from-literal=password='Infra$vm2026!Tr1dent' --dry-run=client -o yaml | oc apply -f -
secret/backend-infra-secret configured
hsuma@plnx-admin:~$
hsuma@plnx-admin:~$
hsuma@plnx-admin:~$ oc -n trident create secret generic backend-app-secret --from-literal=username=vsadmin --from-literal=password='App$vm2026!Tr1dent' --dry-run=client -o yaml | oc apply -f -
secret/backend-app-secret configured
hsuma@plnx-admin:~$ oc -n trident patch tbc backend-infra-nfs --type merge --patch '{"spec":{"managementLIF":"10.172.254.46"}}'
tridentbackendconfig.trident.netapp.io/backend-infra-nfs patched
hsuma@plnx-admin:~$ oc -n trident patch tbc backend-app-nfs --type merge --patch '{"spec":{"managementLIF":"10.172.254.47"}}'
tridentbackendconfig.trident.netapp.io/backend-app-nfs patched
hsuma@plnx-admin:~$ oc get tbc -n trident
NAME                BACKEND NAME        BACKEND UUID                           PHASE   STATUS
backend-app-nfs     backend-app-nfs     27f061fd-e6e4-448c-a4e4-c4302ac4a01a   Bound   Failed
backend-infra-nfs   backend-infra-nfs   88ba74b5-6d56-47cf-a16d-9066ae716681   Bound   Failed
hsuma@plnx-admin:~$ oc describe tbc -n trident backend-app-nfs
Name:         backend-app-nfs
Namespace:    trident
Labels:       <none>
Annotations:  <none>
API Version:  trident.netapp.io/v1
Kind:         TridentBackendConfig
Metadata:
  Creation Timestamp:  2026-02-17T21:31:39Z
  Finalizers:
    trident.netapp.io
  Generation:        2
  Resource Version:  917491
  UID:               df168257-e589-4246-903d-9d8a493252cc
Spec:
  Credentials:
    Name:    backend-app-secret
  Data LIF:  10.172.194.22
  Defaults:
    Export Policy:    default
    Snapshot Policy:  default
    Space Reserve:    none
  Management LIF:     10.172.254.47
  Storage:
    Defaults:
      Snapshot Policy:  default
      Space Reserve:    none
    Labels:
      Environment:  dev
    Defaults:
      Snapshot Policy:  default
      Space Reserve:    none
    Labels:
      Environment:  test
    Defaults:
      Snapshot Policy:  default
      Space Reserve:    volume
    Labels:
      Environment:      prod
  Storage Driver Name:  ontap-nas
  Svm:                  ocp_app_svm
  Version:              1
Status:
  Backend Info:
    Backend Name:         backend-app-nfs
    Backend UUID:         27f061fd-e6e4-448c-a4e4-c4302ac4a01a
  Deletion Policy:        delete
  Last Operation Status:  Failed
  Message:                Failed to apply the backend update; problem initializing storage driver 'ontap-nas': could not configure storage pools: could not get storage pools from array: SVM ocp_app_svm has no assigned aggregates
  Phase:                  Bound
Events:
  Type     Reason  Age                From                    Message
  ----     ------  ----               ----                    -------
  Warning  Failed  40s                trident-crd-controller  Failed to apply the backend update; problem initializing storage driver 'ontap-nas': error initializing ontap-nas driver; could not create Data ONTAP API client: error creating ONTAP API client: error reading SVM details: response code 401 (Unauthorized): incorrect or missing credentials
  Warning  Failed  4s (x24 over 36s)  trident-crd-controller  Failed to apply the backend update; problem initializing storage driver 'ontap-nas': could not configure storage pools: could not get storage pools from array: SVM ocp_app_svm has no assigned aggregates
hsuma@plnx-admin:~$



cat <<EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc-new
  namespace: default
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: netapp-app
EOF



vault secrets enable -path=pki_int pki
vault secrets tune -max-lease-ttl=43800h pki_int


vault write -format=json pki_int/intermediate/generate/internal \
  common_name="Calix OCP-AI Intermediate CA" \
  | jq -r '.data.csr' > /tmp/ocp-ai-intermediate.csr


time="2026-02-18T21:10:22Z" level=info msg="resolving subscriptions in namespace" id=Gs+ao namespace=openshift-gitops-operator
time="2026-02-18T21:10:22Z" level=info msg="unpacking bundles" id=Gs+ao namespace=openshift-gitops-operator
time="2026-02-18T21:10:22Z" level=info msg="unpacking is not complete yet, requeueing" id=Gs+ao namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="resolving sources" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="checking if subscriptions need update" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="checking for existing installplan" channel=latest id=9S1kY namespace=openshift-gitops-operator pkg=openshift-gitops-operator source=redhat-operators sub=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="resolving subscriptions in namespace" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="unpacking bundles" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="unpacking is not complete yet, requeueing" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="resolving sources" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="checking if subscriptions need update" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="checking for existing installplan" channel=latest id=B+keL namespace=openshift-gitops-operator pkg=openshift-gitops-operator source=redhat-operators sub=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="resolving subscriptions in namespace" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="unpacking bundles" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="bundle unpacking failed. Reason: DeadlineExceeded, and Message: Job was active longer than specified deadline" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg=syncing reconciling="*v1alpha1.Subscription" selflink=
time="2026-02-18T21:10:32Z" level=info msg="evaluating current pod" correctHash=true correctImages=true current-pod.name=certified-operators-qf99z current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="of 1 pods matching label selector, 1 have the correct images and matching hash" correctHash=true correctImages=true current-pod.name=certified-operators-qf99z current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="evaluating current pod" correctHash=true correctImages=true current-pod.name=community-operators-2ccwz current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="of 1 pods matching label selector, 1 have the correct images and matching hash" correctHash=true correctImages=true current-pod.name=community-operators-2ccwz current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="evaluating current pod" correctHash=true correctImages=true current-pod.name=redhat-marketplace-xl6qt current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="of 1 pods matching label selector, 1 have the correct images and matching hash" correctHash=true correctImages=true current-pod.name=redhat-marketplace-xl6qt current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="evaluating current pod" correctHash=true correctImages=true current-pod.name=redhat-operators-qvxvw current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="of 1 pods matching label selector, 1 have the correct images and matching hash" correctHash=true correctImages=true current-pod.name=redhat-operators-qvxvw current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="resolving sources" id=YkE7T namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="checking if subscriptions need update" id=YkE7T namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="checking for existing installplan" channel=latest id=YkE7T namespace=openshift-gitops-operator pkg=openshift-gitops-operator source=redhat-operators sub=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="resolving subscriptions in namespace" id=YkE7T namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="unpacking bundles" id=YkE7T namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="bundle unpacking failed. Reason: DeadlineExceeded, and Message: Job was active longer than specified deadline" id=YkE7T namespace=openshift-gitops-operator
hsuma@plnx-admin:~/ocp-ai-config$




oc patch catalogsource redhat-operators -n openshift-marketplace --type merge --patch '{"spec":{"grpcPodConfig":{"memoryTarget":"512Mi"}}}'


Hi Haj
Let’s chat about this.  I want to know if calix.local CA is the right PKI for this.
 
It’s interesting, Frank Ye had a similar request just last week.  This would require IT to allow us to be an issuer for the enterprise PKI (calix.local).  We’ll need to present a strong case for it.
 
I have a questions though.  So please grab some time, maybe Friday morning?  I’d like to understand if / why we want to use calix.local or if it would make more sense to use a self-signed CA.
 
One main question I will have; will any of these certs be used by “Customer” facing applications or is this purely for internal cluster communications?



Hey Brian,

To answer your main question upfront: these certs are purely for internal cluster communications. No customer-facing applications. Specifically, they'd cover the OpenShift API endpoint (api.ocp-ai.calix.local), the web console, and internal application routes (*.apps.ocp-ai.calix.local).

The reason we'd prefer to chain under the Calix CA rather than self-signed is that all internal machines already trust it — so anyone accessing the cluster won't get certificate warnings or need to manually import a new root cert.

We're not looking to become a general issuer for the enterprise PKI. The ask is a single intermediate CA with pathlen:0 (can't create sub-CAs), scoped to our OCP-AI cluster and managed through HashiCorp Vault with automated issuance and rotation.



Hey Sulabh, to follow up on your question about CNPG and vector databases — yes, CNPG supports vector databases through the pgvector extension on PostgreSQL. We can set that up on the new OCP-AI cluster.

We're building out the AI platform on-prem now and I want to make sure we match what your team needs. Can you share what your current stack looks like on GCP? Specifically:

- ML platform (Vertex AI, Kubeflow, etc.)
- Notebook environment
- Model serving framework
- Object storage
- Databases (relational + vector)

That way I can set up the same or similar services on OpenShift. Thanks!





Hey Sulabh,

To follow up on your question about CNPG and vector databases — yes, CNPG supports vector databases through the pgvector extension on PostgreSQL. We can set that up on the new OCP-AI cluster.

I also know you mentioned your team is using MinIO. We're planning to deploy MinIO on the cluster as well for S3-compatible object storage backed by NetApp.

We're building out the full AI platform on-prem now and I want to make sure we match what your team needs. Can you share what your current stack looks like on GCP? Specifically:

- ML platform (Vertex AI, Kubeflow, etc.)
- Notebook environment
- Model serving framework
- Databases (relational + vector)
- Any other services your workflows depend on

That way I can set up the same or similar services on OpenShift.

Thanks!


Ok.  Glad to hear that.  I don’t know if IT will have the right certificate template for an intermediate/issuer CA.
 
Can you share the details of the CSR you sent over?  I’m just curious.
 
Also, just want to keep in mind we’ll need to renew that well ahead of time and know that IT are not PKI experts today.
 
From: Haj Suma <haj.suma@calix.com>
Sent: Wednesday, February 18, 2026 2:48 PM
To: Brian Wing <Brian.Wing@calix.com>; Deonna Thornton <deonna.thornton@calix.com>
Subject: Re: CSR Signing Request - OCP-AI Vault Intermediate CA
 
No I am not – I can generate self-signed cert while I wait on that
 
From: Brian Wing <Brian.Wing@calix.com>
Date: Wednesday, February 18, 2026 at 5:45 PM
To: Haj Suma <haj.suma@calix.com>, Deonna Thornton <deonna.thornton@calix.com>
Subject: RE: CSR Signing Request - OCP-AI Vault Intermediate CA

oK, I do understand.  Let’s still connect, IT will need a good “explaining to” to understand the request.  Are you blocked at this point?


Hi Haj - I will respond to you with other details by Friday but like to call out that please do not setup pgVector. 
We don’t use pgVector instead we use milvus as our vector store. 

Thank you for support in setting the infrastructure.

Best Regards,
Sulabh Sinha
M: 669-660-1974



Hey Sulabh,

Good to know — noted on Milvus instead of pgVector. I'll plan for Milvus as the vector store.

No rush on the other details, Friday works. Looking forward to the full picture so we can get everything set up for your team.

Thanks!




Hi Haj,

Please confirm if any tasks are currently assigned to Vinoth and share the timelines for those tasks.

Thanks.


 
Best,
Kishore Kumar AJ



Hi Kishore,

Thanks for checking in. At this point, no tasks have been assigned to Vinoth. The core infrastructure build (cluster deployment, storage networking, GPU integration, Trident CSI, image registry) has been completed and is well underway.

The remaining work involves tightly integrated configuration tasks that require deep context of the environment and decisions made during the build. I want to make sure we maintain consistency and avoid any rework.

That said, I'd welcome support in areas where clear, well-scoped tasks can be carved out. Once we get past the current phase, there will be opportunities around monitoring dashboards, logging configuration, and documentation that would be good candidates.

I'll keep you posted as those open up.

Thanks!


 [core@ocp-ai-master02 ~]$
[core@ocp-ai-master02 ~]$ ip link show eno7
4: eno7: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 9000 qdisc mq master bnd0 state UP mode DEFAULT group default qlen 1000
    link/ether 00:25:b5:50:0a:67 brd ff:ff:ff:ff:ff:ff
    altname enp27s0f2
    altname ens7f2
---
[systemd]
Failed Units: 1
  NetworkManager-wait-online.service
[core@ocp-ai-master03 ~]$ ip link show eno7
4: eno7: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 9000 qdisc mq master bnd0 state UP mode DEFAULT group default qlen 1000
    link/ether 00:25:b5:50:0a:69 brd ff:ff:ff:ff:ff:ff
    altname enp27s0f2
    altname ens7f2


hsuma@plnx-admin:~$ ssh -i ~/.ssh/ocp-ai core@10.172.248.154
Red Hat Enterprise Linux CoreOS 9.6.20260128-1
  Part of OpenShift 4.20, RHCOS is a Kubernetes-native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).



---

[core@ocp-ai-gpu02 ~]$ ip link show eno8303
2: eno8303: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
    link/ether c8:4b:d6:f3:d8:22 brd ff:ff:ff:ff:ff:ff
    altname enp2s0f0
[core@ocp-ai-gpu02 ~]$
[core@ocp-ai-gpu02 ~]$ ip link show ens8f1np1
7: ens8f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 7c:8c:09:c3:79:87 brd ff:ff:ff:ff:ff:ff
    altname enp160s0f1np1

[core@ocp-ai-gpu02 ~]$ sudo cat /sys/class/dmi/id/product_serial
7RJWMD4
[core@ocp-ai-gpu02 ~]$



This page tracks the platform build-out tasks required to make the UCS-X-AI environment ready for workloads.

TASK 1: OpenShift Installation - 95% COMPLETE (Est: 5 days | Remaining: 0.5 day)

Subtask

Description

Est.

Status

Notes

1.1 

Generate discovery ISO via Assisted Installer 

0.5d 

Done 

Full image, OCP 4.20.13 

1.2 

Configure Intersight vMedia / boot media 

1d 

Done 

Used KVM virtual media for UCS-X, iDRAC virtual media for Dell 

1.3 

Boot blades and assign roles 

0.5d 

Done 

3 masters operational, 2 GPU workers installing 

1.4 

Complete OpenShift installation and bootstrap 

1.5d 

Done 

Cluster bootstrapped and operational 

1.5 

Validate all nodes Ready and operators healthy 

0.5d 

Done 

3/3 masters Ready, all operators healthy, 223 pods 

1.6 

Configure kubeadmin credentials and oc CLI 

0.5d 

Done 

oc CLI on plnx-admin, kubeconfig downloaded 

1.7 

Configure NTP sync across all nodes 

0.5d 

Open 

Not yet configured 

Remaining: NTP configuration (1.7), GPU worker CSR approval (pending install completion)

TASK 2: Dell GPU Server Setup - 75% COMPLETE (Est: 4 days | Remaining: 1 day)

Subtask

Description

Est.

Status

Notes

2.1 

Network connectivity on VLAN 1510 

2d 

Done 

Chetan configured VLAN 1510 on Leaf-3 switch 

2.2 

Boot Discovery ISO and register hosts 

0.5d 

Done 

Both hosts discovered in Assisted Installer 

2.3 

Set hostnames and roles 

0.25d 

Done 

gpu01/gpu02 set as Worker role 

2.4 

Install CoreOS and join cluster 

0.5d 

In Progress 

Installation started, awaiting completion 

2.5 

Approve worker CSRs 

0.25d 

Open 

Pending install completion 

Hardware Confirmed (per server):

4x NVIDIA H100L 94GB GPUs (8 total across both servers)

128 cores (hyper-threaded) Intel Xeon Platinum 8562Y+

2 TiB RAM

480 GB NVMe boot drive (Dell BOSS-N1)

26.88 TB RAID array (local scratch storage)

Mellanox ConnectX-6 Dx (40Gbps + 100Gbps)

Updated Notes: Original subtasks 2.1-2.4 (manual RHEL install, GPU drivers, CUDA toolkit) are NOT required — CoreOS is installed automatically by Assisted Installer, GPU drivers handled by NVIDIA GPU Operator (Task 8).

TASK 3: Storage Networking - 95% COMPLETE (Est: 3 days | Remaining: 0.5 day)

Subtask

Description

Est.

Status

Notes

3.1 

Create MachineConfigs for VLAN subinterfaces 

1d 

Done 

NMState NNCPs with bond (bnd0 = eno7 + eno8 active-backup) 

3.2 

Configure infra/app VLANs for dev/test/prod 

0.5d 

Done 

6 VLANs: 1792, 1793, 1794, 1796, 1797, 1798 

3.3 

Assign IPs and set MTU 9000 

0.5d 

Done 

IPs .150/.151/.152 per VLAN, MTU 9000 on bond 

3.4 

Validate connectivity and jumbo frames to NetApp LIFs 

0.5d 

Open 

Ping works, jumbo frame test not yet run 

Remaining: Jumbo frame validation (ping -M do -s 8972)

TASK 4: NetApp Trident CSI - 90% COMPLETE (Est: 3 days | Remaining: 0.5 day)

Subtask

Description

Est.

Status

Notes

4.1 

Install Trident operator 

0.5d 

Done 

NetApp Trident Certified from OperatorHub 

4.2 

Create Trident backends for infra and app SVMs 

1d 

Done 

backend-infra-nfs and backend-app-nfs — Bound/Success 

4.3 

Create StorageClasses per environment 

0.5d 

Done 

netapp-infra, netapp-app (default) 

4.4 

Validate PVC creation and pod mounts 

0.5d 

Done 

PVC test successful — Bound 

4.5 

Validate read/write access to NetApp 

0.5d 

Open 

Pod mount read/write test not yet performed 

Pending from Deonna: ALL RESOLVED

Fix prod LIF VLANs (1800→1793, 1802→1797)

Create SVM management LIFs 

Create SVM service accounts 

Configure NFS export policies  Done

Action needed: Update Trident backend configs with new SVM mgmt LIFs and service account credentials

TASK 5: Internal Image Registry - NOT STARTED (Est: 1 day)

Subtask

Description

Est.

Status

Notes

5.1 

Enable OpenShift internal registry 

0.25d 

Open 



5.2 

Back registry storage with NetApp via Trident 

0.25d 

Open 



5.3 

Validate image push/pull 

0.25d 

Open 



5.4 

Configure image pull secrets 

0.25d 

Open 



Dependencies: Trident operational  — no blockers

TASK 6: Ingress and Load Balancing - NOT STARTED (Est: 3 days)

Subtask

Description

Est.

Status

Notes

6.1 

Deploy MetalLB or configure F5 integration 

1d 

 Open 



6.2 

Define IP pools and ingress configuration 

0.5d 

 Open 



6.3 

Configure DNS wildcard records 

0.5d 

 Open 



6.4 

Configure TLS certificates 

0.5d 

 Open 



6.5 

Validate external access 

0.5d 

 Open 



Dependencies: None — can proceed now

TASK 7: Authentication and RBAC - NOT STARTED (Est: 4 days)

Subtask

Description

Est.

Status

Notes

7.1 

Configure identity provider (IPA/LDAP) 

1d 

 Open 

We manage IPA — no external dependency. Okta can be added later as second IdP if needed

7.2 

Define RBAC for admin, dev, and ops roles 

1d 

 Open 

Map LDAP groups to OCP ClusterRoles/RoleBindings 

7.3 

Create dev/test/prod namespaces 

0.5d 

Open 



7.4 

Apply quotas, limits, and network policies 

0.5d 

 Open 



7.5 

Add Okta as secondary IdP (if approved) 

1d 

 Pending 

Awaiting manager decision — does not block other subtasks 

Dependencies: None 

TASK 8: GPU Operator and Scheduling - NOT STARTED (Est: 2 days | Blocked on Task 2)

Subtask

Description

Est.

Status

Notes

8.1 

Install Node Feature Discovery 

0.25d 

 Open 



8.2 

Install NVIDIA GPU Operator 

0.5d 

 Open 



8.3 

Configure GPU scheduling 

0.5d 

Open 



8.4 

Apply taints/tolerations for GPU nodes 

0.25d 

 Open 



8.5 

Validate GPU access from pods 

0.5d 

 Open 



Dependencies: GPU workers must be joined to cluster (Task 2.4/2.5)
Hardware: 8x NVIDIA H100L 94GB total (4 per server)

TASK 9: GitOps - 10% COMPLETE (Est: 4 days | Remaining: 3.5 days)

Subtask

Description

Est.

Status

Notes

9.1 

Install OpenShift GitOps (ArgoCD) 

0.5d 

 Open 



9.2 

Configure ArgoCD and RBAC 

1d 

 Open 



9.3 

Define repository structure 

1d 

 Started 

Local repo at ~/ocp-ai-config with nmstate/ and trident/ 

9.4 

Bootstrap cluster configuration via GitOps 

1d 

Open 



9.5 

Validate sync and rollback 

0.5d 

 Open 



Dependencies: Bitbucket Stash repo creation pending

TASK 10: Backup and Disaster Recovery - NOT STARTED (Est: 3 days)

Subtask

Description

Est.

Status

Notes

10.1 

Install Velero or OADP 

0.5d 

Open 



10.2 

Configure backup storage location 

0.5d 

Open 



10.3 

Define backup schedules 

0.5d 

 Open 



10.4 

Configure etcd backup automation 

0.5d 

 Open 



10.5 

Test restore of a sample namespace 

1d 

 Open 



Dependencies: Storage classes operational 

TASK 11: End-to-End Validation - NOT STARTED (Est: 4 days)

Subtask

Description

Est.

Status

Notes

11.1 

Deploy sample AI workload (GPU + NetApp) 

1.5d 

 Open 



11.2 

Validate deployment via GitOps 

1d 

Open 



11.3 

Validate dev/test/prod isolation 

0.5d 

 Open 



11.4 

Validate backup and restore of workload 

1d 

 Open 



Dependencies: Tasks 2, 8, 9, 10 must be complete

TASK 12: Monitoring and Alerting - FUTURE / ADD-ON ()

Subtask

Description

Est.

Status

Notes

12.1 

Configure persistent storage for monitoring 

0.5d 

 Open 



12.2 

Define retention policies 

0.5d 

 Open 



12.3 

Create custom alert rules 

1d 

 Open 



12.4 

Create dashboards 

1d 

 Open 



TASK 13: Logging - FUTURE / ADD-ON ()

Subtask

Description

Est.

Status

Notes

13.1 

Deploy logging stack 

0.5d 

Open 



13.2 

Configure persistent storage 

0.5d 

 Open 



13.3 

Define retention policies 

0.5d 

 Open 



13.4 

Configure log forwarding 

0.5d 

 Open 





TASK 14: IPAM - Leverage IPAM Solution (Est: 2 days)

Subtask

Description

Est.

Status

Notes

14.1 

Deploy logging stack 

0.5d 

Open 



13.2 

Configure persistent storage 

0.5d 

 Open 



13.3 

Define retention policies 

0.5d 

 Open 



13.4 

Configure log forwarding 

0.5d 

 Open 





Overall Progress Summary

Task

Description

Status

%

Est. Days

Days Remaining

1 

OpenShift Installation 

Nearly Complete 

95% 

5 

0.5 

2 

Dell GPU Server Setup 

In Progress 

75% 

4 

1 

3 

Storage Networking 

Nearly Complete 

95% 

3 

0.5 

4 

NetApp Trident CSI 

Nearly Complete 

90% 

3 

0.5 

5 

Internal Image Registry 

Not Started 

0% 

1 

1 

6 

Ingress and Load Balancing 

Not Started 

0% 

3 

3 

7 

Authentication and RBAC 

Not Started 

0% 

4 

4 

8 

GPU Operator and Scheduling 

Blocked on Task 2 

0% 

2 

2 

9 

GitOps 

Started 

10% 

4 

3.5 

10 

Backup and DR 

Not Started 

0% 

3 

3 

11 

End-to-End Validation 

Not Started 

0% 

4 

4 

12 

Monitoring (future) 

Not Started 

0% 

3 

3 

13 

Logging (future) 

Not Started 

0% 

2 

2 



Key Milestones

Milestone

Date

Description

Engineering Complete 

March 18, 2026 

All 13 tasks completed 

Shakeout Testing 

March 19–20, 2026 

2-day stress test, failure injection, workload validation 

Bug Fixes / Hardening 

March 21–22, 2026 

Resolve any issues found during shakeout 

Cluster Release 

March 23, 2026 

Cluster released for production deployment 

Dependency Tracker

Dependency

Owner

Status

Impact

VLAN 1510 on Leaf-3 

Chetan 

 Resolved 

GPU workers can now join 

NetApp prod VLAN fix (1800/1802→1793/1797) 

Deonna Thornton 

 Resolved 

VLANs corrected to 1793/1797 

SVM management LIFs 

Deonna Thornton 

 Resolved 

Need to get IPs and update Trident backends 

SVM service accounts 

Deonna Thornton 

 Resolved 

Need to get creds and update Trident secrets 

NFS export policies 

Deonna Thornton 

 Resolved 

Configured 

Bitbucket Stash repo 

Haj 

 Pending 

Blocks Task 9 push 

Red Hat subscription 

Haj 

 Pending 

Trial expires ~April 12 





Hi Kishore,

Thanks for checking in. The core infrastructure build (cluster deployment, storage networking, GPU integration, Trident CSI, image registry) has been completed and is well underway. The remaining work involves tightly integrated configuration tasks that require deep context of the environment and decisions made during the build. I want to make sure we maintain consistency and avoid any rework. That said, I'd welcome support in areas where clear, well-scoped tasks can be carved out.

I've assigned Vinoth to Task 15: NetBox IPAM — OCP-AI Infrastructure Documentation.

I've already gathered and documented all the network, IP, and NetApp details (subtasks 15.1–15.3). His scope is:

- Enter all data into the existing NetBox instance
- Document rack layouts and hardware inventory
- Document switch port mappings and cable connections (coordinate with Chetan/Gowtham)
- Validate all entries with me

Estimated duration: 1 week




Hey Chetan,

The GPU workers (ocp-ai-gpu01/gpu02) are currently running on the 1Gbps Broadcom NIC (eno8303). We need to migrate them to the Mellanox ConnectX-6 100GbE NIC for better throughput.

Can you configure the following on the Mellanox switch ports for both Dell servers?

- VLAN 1510 (native/untagged)
- MTU 9216

Here are the Mellanox Port 2 MACs:
- gpu01 (ens8f1np1): 7C:8C:09:C3:74:1F
- gpu02 (ens8f1np1): 7C:8C:09:C3:79:87

Let me know when you can get to this and I'll coordinate the cutover on our side.





cat <<'EOF' | oc apply -f -
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-master-chrony
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,c2VydmVyIHRpbWUxLmNhbGl4LmxvY2FsIGlidXJzdApzZXJ2ZXIgdGltZTIuY2FsaXgubG9jYWwgaWJ1cnN0CmRyaWZ0ZmlsZSAvdmFyL2xpYi9jaHJvbnkvZHJpZnQKbWFrZXN0ZXAgMS4wIDMKcnRjc3luYwpsb2dkaXIgL3Zhci9sb2cvY2hyb255Cg==
        mode: 0644
        overwrite: true
        path: /etc/chrony.conf
EOF




cat <<'EOF' | oc apply -f -
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 99-worker-chrony
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,c2VydmVyIHRpbWUxLmNhbGl4LmxvY2FsIGlidXJzdApzZXJ2ZXIgdGltZTIuY2FsaXgubG9jYWwgaWJ1cnN0CmRyaWZ0ZmlsZSAvdmFyL2xpYi9jaHJvbnkvZHJpZnQKbWFrZXN0ZXAgMS4wIDMKcnRjc3luYwpsb2dkaXIgL3Zhci9sb2cvY2hyb255Cg==
        mode: 0644
        overwrite: true
        path: /etc/chrony.conf
EOF


ssh -i ~/.ssh/ocp-ai core@10.172.248.150 chronyc sources




Sure @Chetan Kudari, I would suggest you check, those MAC addresses may already be connected.  But we should ensure they are.
 
@Haj Suma I thought the Dell systems may have a dual port NIC (100Gbps).  Can we set up a redundant connection for these?
 
Ideally we would have a meshed connectivity for 100G from the Dell servers to both Leaf 5 and Leaf 6 so we could have redundancy and auto failover, or something like that.  We should try and make that work if we have the ports and interfaces.




Hey Brian,

Yes — each Dell server has a dual-port Mellanox ConnectX-6 Dx in Slot 8. Both ports are cabled and showing UP:

- Port 1 (ens8f0np0): 40GbE
- Port 2 (ens8f1np1): 100GbE

I can set up redundancy with an active-backup bond on the OS side across both ports. If Chetan can get each port connected to a different leaf (one on Leaf-5, one on Leaf-6), we'll have automatic failover if either link or switch goes down.

MACs for both servers:

gpu01:
- Port 1: 7C:8C:09:C3:74:1E
- Port 2: 7C:8C:09:C3:74:1F

gpu02:
- Port 1: need to confirm from iDRAC
- Port 2: 7C:8C:09:C3:79:87

@Chetan can you check what each port is connected to and whether we can get one port on Leaf-5 and the other on Leaf-6? Both ports need VLAN 1510 (native/untagged) and MTU 9216.



NIC Slot 8: Mellanox ConnectX-6 Dx Dual Port 100 GbE QSFP56 Adapter - 7C:8C:09:C3:79:86   
Product Name	Mellanox ConnectX-6 Dx Dual Port 100 GbE QSFP56 Adapter - 7C:8C:09:C3:79:86
Vendor Name	Mellanox Technologies, Inc.
Number of Ports	2
SNAPI State	Disabled
SNAPI Support	Available
VPI Support	Not Available
Summary
Port 1
Port 2
 Refresh Connection View
Port	Link Status	Link Speed	Protocol	Switch Connection ID	Switch Port Connection ID	CPU Affinity
1	Up	100 Gbps	NIC,RDMA	48:80:02:e3:bd:98	Ethernet1/13	2
2	Up	100 Gbps	NIC,RDMA	a0:bc:6f:d9:73:00	Ethernet1/13	2



 NIC Slot 8: Mellanox ConnectX-6 Dx Dual Port 100 GbE QSFP56 Adapter - 7C:8C:09:C3:74:1E   
Product Name	Mellanox ConnectX-6 Dx Dual Port 100 GbE QSFP56 Adapter - 7C:8C:09:C3:74:1E
Vendor Name	Mellanox Technologies, Inc.
Number of Ports	2
SNAPI State	Disabled
SNAPI Support	Available
VPI Support	Not Available
Summary
Port 1
Port 2
 Refresh Connection View
Port	Link Status	Link Speed	Protocol	Switch Connection ID	Switch Port Connection ID	CPU Affinity
1	Up	40 Gbps	NIC,RDMA	a0:bc:6f:d9:72:fc	Ethernet1/12	2
2	Up	100 Gbps	NIC,RDMA	48:80:02:e3:bd:94	Ethernet1/12	2




Hey Brian,

Confirmed from iDRAC — each Dell server has a dual-port Mellanox ConnectX-6 Dx 100GbE in Slot 8. Both ports are UP and already connected to two different switches, so we have the physical redundancy in place. I can set up an active-backup bond on the OS side for automatic failover.

gpu01:
- Port 1: 7C:8C:09:C3:74:1E — 40Gbps — switch a0:bc:6f:d9:72:fc Eth1/12
- Port 2: 7C:8C:09:C3:74:1F — 100Gbps — switch 48:80:02:e3:bd:94 Eth1/12

gpu02:
- Port 1: 7C:8C:09:C3:79:86 — 100Gbps — switch 48:80:02:e3:bd:98 Eth1/13
- Port 2: 7C:8C:09:C3:79:87 — 100Gbps — switch a0:bc:6f:d9:73:00 Eth1/13

Note: gpu01 Port 1 is negotiating at 40Gbps instead of 100Gbps — may be a cable or switch port config issue.

@Chetan can you configure VLAN 1510 (native/untagged) and MTU 9216 on all four ports above? Also can you check why gpu01 Port 1 is at 40Gbps instead of 100Gbps?

Thanks!


I thought that might be the case.  Work with @Chetan Kudari to define what kind of redundancy you can establish, like an LACP or some other protocol of LAG.  The nexus should probably be able to support anything the Linux OS can do.



Hey Brian,

FYI — NTP is configured and syncing on all OCP-AI nodes using time1.calix.local (10.168.21.1) and time2.calix.local (10.168.21.2).

Not a blocker, but noticed those IPs also have a DNS entry under caal.dev (time1.caal.dev resolves to the same IP). Just a cosmetic thing if anyone ever wants to clean that up in AD DNS.



Hi Haj,

Thanks for the MAC details. As verified, these are connected to the switch but no configuration has been pushed from it. Could you please confirm if VLANs 1792-1794, 1796-1798, and 1510 should be allowed on these ports? Also, should all 4 ports be on a single LAG or do you want 2 LAGs?

Best regards,

Chetan




Subject: Re: GPU Worker NIC Migration — 1G to 100GbE

Hey Chetan,

Yes — go ahead and allow VLANs 1510, 1792, 1793, 1794, 1796, 1797, and 1798 on all four ports. VLAN 1510 as native/untagged, the rest tagged.

For LAG — 2 separate LAGs, one per server:
- LAG 1: gpu01 Port 1 + Port 2
- LAG 2: gpu02 Port 1 + Port 2

LACP (802.3ad) on both. MTU 9216 on all ports.

For the gpu01 Port 1 40Gbps issue — it needs to be connected to a 100G transceiver.

Let me know once everything is configured and I'll start on the OS side.

Thanks!


cat <<'EOF' | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-rw
  namespace: default
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: netapp-app
---
apiVersion: v1
kind: Pod
metadata:
  name: test-rw-pod
  namespace: default
spec:
  containers:
  - name: test
    image: busybox
    command: ["sh", "-c", "echo 'write test ok' > /mnt/testfile && cat /mnt/testfile && sleep 10"]
    volumeMounts:
    - name: testvol
      mountPath: /mnt
  restartPolicy: Never
  volumes:
  - name: testvol
    persistentVolumeClaim:
      claimName: test-rw
EOF


hsuma@plnx-admin:~/ocp-ai-config$ git push -u origin main
error: src refspec main does not match any
error: failed to push some refs to 'bitbucket.org:calixprod/ocp-ai-platform.git'
hsuma@plnx-admin:~/ocp-ai-config$



hsuma@plnx-admin:~/ocp-ai-config$ git push -u origin main
Enter passphrase for key '/home/hsuma/.ssh/id_ed25519':
Enter passphrase for key '/home/hsuma/.ssh/id_ed25519':
To bitbucket.org:calixprod/ocp-ai-platform.git
 ! [rejected]        main -> main (fetch first)
error: failed to push some refs to 'bitbucket.org:calixprod/ocp-ai-platform.git'
hint: Updates were rejected because the remote contains work that you do
hint: not have locally. This is usually caused by another repository pushing
hint: to the same ref. You may want to first integrate the remote changes
hint: (e.g., 'git pull ...') before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.
hsuma@plnx-admin:~/ocp-ai-config$



oc -n openshift-gitops create secret generic repo-ocp-ai-platform \
  --from-literal=url=git@bitbucket.org:calixprod/ocp-ai-platform.git \
  --from-file=sshPrivateKey=$HOME/.ssh/argocd-deploy-key \
  --from-literal=type=git \
  --from-literal=insecure=false

oc -n openshift-gitops label secret repo-ocp-ai-platform argocd.argoproj.io/secret-type=repository
