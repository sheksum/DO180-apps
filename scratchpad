cat <<EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: image-registry-storage
  namespace: openshift-image-registry
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Gi
  storageClassName: netapp-infra
EOF

oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"managementState":"Managed","storage":{"pvc":{"claim":"image-registry-storage"}},"replicas":2}}'



oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"defaultRoute":true}}'


oc get route -n openshift-image-registry


HOST=$(oc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}')
podman login -u kubeadmin -p $(oc whoami -t) $HOST
podman pull busybox
podman tag busybox $HOST/default/busybox:test
podman push $HOST/default/busybox:test


hsuma@plnx-admin:~$ podman login -u kubeadmin -p $(oc whoami -t) $HOST --tls-verify=false
error: no token is currently in use for this session
ERRO[0000] cannot find UID/GID for user hsuma: No subuid ranges found for user "hsuma" in /etc/subuid - check rootless mode in man pages.
WARN[0000] using rootless single mapping into the namespace. This might break some images. Check /etc/subuid and /etc/subgid for adding sub*ids
Error: no registries found in registries.conf, a registry must be provided
hsuma@plnx-admin:~$


oc rsh -n openshift-image-registry $(oc get pods -n openshift-image-registry -l docker-registry=default -o jsonpath='{.items[0].metadata.name}') touch /registry/test-write && echo "Write OK"



hsuma@plnx-admin:~$ oc rsh -n openshift-image-registry $(oc get pods -n openshift-image-registry -l docker-registry=default -o jsonpath='{.items[0].metadata.name}') touch /registry/test-write && echo "Write OK"
^Ccommand terminated with exit code 130
hsuma@plnx-admin:~$
hsuma@plnx-admin:~$
hsuma@plnx-admin:~$ oc describe pvc image-registry-storage -n openshift-image-registry
Name:          image-registry-storage
Namespace:     openshift-image-registry
StorageClass:  netapp-infra
Status:        Bound
Volume:        pvc-766e03dc-d5e4-496f-9bd0-8b38905999dd
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-provisioner: csi.trident.netapp.io
               volume.kubernetes.io/storage-provisioner: csi.trident.netapp.io
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      100Gi
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       image-registry-5b49fc8797-7m7d4
               image-registry-5b49fc8797-mqmsp
Events:
  Type    Reason                 Age                From                                                                                            Message
  ----    ------                 ----               ----                                                                                            -------
  Normal  Provisioning           21m                csi.trident.netapp.io_trident-controller-7cb57b6f6d-vgf24_3b4a98db-5441-47a5-abfc-6e7ade299819  External provisioner is provisioning volume for claim "openshift-image-registry/image-registry-storage"
  Normal  ExternalProvisioning   21m (x2 over 21m)  persistentvolume-controller                                                                     Waiting for a volume to be created either by the external provisioner 'csi.trident.netapp.io' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
  Normal  ProvisioningSuccess    21m                csi.trident.netapp.io                                                                           provisioned a volume
  Normal  ProvisioningSucceeded  21m                csi.trident.netapp.io_trident-controller-7cb57b6f6d-vgf24_3b4a98db-5441-47a5-abfc-6e7ade299819  Successfully provisioned volume pvc-766e03dc-d5e4-496f-9bd0-8b38905999dd
hsuma@plnx-admin:~$


plano_aff_cl::> volume show -vserver ocp_infra_svm
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
ocp_infra_svm
          ocp_infra_svm_root
                       aggr1_n5     online     RW          1GB    970.9MB    0%

plano_aff_cl::>
plano_aff_cl::>
plano_aff_cl::> volume show -vserver ocp_app_svm
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
ocp_app_svm
          ocp_app_svm_root
                       aggr1_n6     online     RW          1GB    970.8MB    0%
ocp_app_svm
          trident_pvc_131edc3f_480b_47dd_bc98_b077bd05b17b
                       aggr1_n4     online     RW       1.05GB     1023MB    0%
ocp_app_svm
          trident_pvc_766e03dc_d5e4_496f_9bd0_8b38905999dd
                       aggr1_n5     online     RW      105.3GB   100.00GB    0%
3 entries were displayed.

plano_aff_cl::>


plano_aff_cl::> export-policy rule show -vserver ocp_app_svm -fields rorule,rwrule,superuser
vserver     policyname         ruleindex rorule rwrule superuser
----------- ------------------ --------- ------ ------ ---------
ocp_app_svm backend_app_access 1         any    any    any
ocp_app_svm backend_app_access 2         any    any    any
ocp_app_svm backend_app_access 3         any    any    any
ocp_app_svm backend_app_access 4         any    any    any
ocp_app_svm default            1         any    any    any
5 entries were displayed.

plano_aff_cl::>


plano_aff_cl::> export-policy rule show -vserver ocp_infra_svm
             Policy          Rule    Access   Client                RO
Vserver      Name            Index   Protocol Match                 Rule
------------ --------------- ------  -------- --------------------- ---------
ocp_infra_svm
             backend_infra_access
                             1       nfs3,    10.172.192.0/24       any
                                     nfs4,
                                     nfs
ocp_infra_svm
             backend_infra_access
                             2       nfs3,    10.172.196.0/24       any
                                     nfs4,
                                     nfs
ocp_infra_svm
             backend_infra_access
                             3       nfs3,    10.172.200.0/24       any
                                     nfs4,
                                     nfs
ocp_infra_svm
             backend_infra_access
                             4       nfs3,    10.172.248.0/24       any
                                     nfs4,
                                     nfs
ocp_infra_svm
             default         2       nfs3,    0.0.0.0/0             any
                                     nfs4,
                                     nfs
5 entries were displayed.

plano_aff_cl::> export-policy rule show -vserver ocp_app_svm
             Policy          Rule    Access   Client                RO
Vserver      Name            Index   Protocol Match                 Rule
------------ --------------- ------  -------- --------------------- ---------
ocp_app_svm  backend_app_access
                             1       nfs3,    10.172.194.0/24       any
                                     nfs4,
                                     nfs
ocp_app_svm  backend_app_access
                             2       nfs3,    10.172.198.0/24       any
                                     nfs4,
                                     nfs
ocp_app_svm  backend_app_access
                             3       nfs3,    10.172.202.0/24       any
                                     nfs4,
                                     nfs
ocp_app_svm  backend_app_access
                             4       nfs3,    10.172.248.0/24       any
                                     nfs4,
                                     nfs
ocp_app_svm  default         1       nfs3,    0.0.0.0/0             any
                                     nfs4,
                                     nfs
5 entries were displayed.

plano_aff_cl::>



 plano_aff_cl::> export-policy rule show -vserver ocp_app_svm -fields rorule,rwrule,superuser
vserver     policyname         ruleindex rorule rwrule superuser
----------- ------------------ --------- ------ ------ ---------
ocp_app_svm backend_app_access 1         any    any    any
ocp_app_svm backend_app_access 2         any    any    any
ocp_app_svm backend_app_access 3         any    any    any
ocp_app_svm backend_app_access 4         any    any    any
ocp_app_svm default            1         any    any    any
5 entries were displayed.

plano_aff_cl::>


[core@ocp-ai-master02 ~]$ mount | grep trident
10.172.194.22:/trident_pvc_766e03dc_d5e4_496f_9bd0_8b38905999dd on /var/lib/kubelet/pods/b1e78fdd-8266-4bd0-af4a-ae362b44a2e0/volumes/kubernetes.io~csi/pvc-766e03dc-d5e4-496f-9bd0-8b38905999dd/mount type nfs4 (rw,relatime,vers=4.1,rsize=65536,wsize=65536,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.172.194.151,local_lock=none,addr=10.172.194.22)
[core@ocp-ai-master02 ~]$


oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"nodeSelector":{"node-role.kubernetes.io/master":""}}}'



plano_aff_cl::> network interface show -vserver ocp_infra_svm
            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
ocp_infra_svm
            ocp_infra_dev_lif01
                         up/up    10.172.192.22/23   plano_aff_cl-05
                                                                   a0a-1792
                                                                           true
            ocp_infra_dev_lif02
                         up/up    10.172.192.23/23   plano_aff_cl-06
                                                                   a0a-1792
                                                                           true
            ocp_infra_mgmt
                         up/up    10.172.254.46/24   plano_aff_cl-05
                                                                   a0a-1570
                                                                           true
            ocp_infra_prod_lif01
                         up/up    10.172.200.22/23   plano_aff_cl-05
                                                                   a0a-1793
                                                                           true
            ocp_infra_prod_lif02
                         up/up    10.172.200.23/23   plano_aff_cl-06
                                                                   a0a-1793
                                                                           true
            ocp_infra_test_lif01
                         up/up    10.172.196.22/23   plano_aff_cl-05
                                                                   a0a-1796
                                                                           true
            ocp_infra_test_lif02
                         up/up    10.172.196.23/23   plano_aff_cl-06
                                                                   a0a-1796
                                                                           true
7 entries were displayed.

plano_aff_cl::> network interface show -vserver ocp_app_svm
            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
ocp_app_svm
            ocp_app_dev_lif01
                         up/up    10.172.194.22/23   plano_aff_cl-05
                                                                   a0a-1794
                                                                           true
            ocp_app_dev_lif02
                         up/up    10.172.194.23/23   plano_aff_cl-06
                                                                   a0a-1794
                                                                           true
            ocp_app_mgmt up/up    10.172.254.47/24   plano_aff_cl-06
                                                                   a0a-1570
                                                                           true
            ocp_app_prod_lif01
                         up/up    10.172.202.22/23   plano_aff_cl-05
                                                                   a0a-1797
                                                                           true
            ocp_app_prod_lif02
                         up/up    10.172.202.23/23   plano_aff_cl-06
                                                                   a0a-1797
                                                                           true
            ocp_app_test_lif01
                         up/up    10.172.198.22/23   plano_aff_cl-05
                                                                   a0a-1798
                                                                           true
            ocp_app_test_lif02
                         up/up    10.172.198.23/23   plano_aff_cl-06
                                                                   a0a-1798
                                                                           true
7 entries were displayed.

plano_aff_cl::>


lano_aff_cl::> security login show -vserver ocp_infra_svm

Vserver: ocp_infra_svm
                                                                 Second
User/Group                 Authentication                 Acct   Authentication
Name           Application Method        Role Name        Locked Method
-------------- ----------- ------------- ---------------- ------ --------------
vsadmin        http        password      vsadmin          yes    none
vsadmin        ontapi      password      vsadmin          yes    none
vsadmin        ssh         password      vsadmin          yes    none
3 entries were displayed.

plano_aff_cl::>
plano_aff_cl::> security login show -vserver ocp_app_svm

Vserver: ocp_app_svm
                                                                 Second
User/Group                 Authentication                 Acct   Authentication
Name           Application Method        Role Name        Locked Method
-------------- ----------- ------------- ---------------- ------ --------------
vsadmin        http        password      vsadmin          yes    none
vsadmin        ontapi      password      vsadmin          yes    none
vsadmin        ssh         password      vsadmin          yes    none
3 entries were displayed.

plano_aff_cl::>




oc -n trident create secret generic backend-infra-secret --from-literal=username=vsadmin --from-literal=password='<infra-password>' --dry-run=client -o yaml | oc apply -f -

oc -n trident create secret generic backend-app-secret --from-literal=username=vsadmin --from-literal=password='<app-password>' --dry-run=client -o yaml | oc apply -f -

oc -n trident patch tbc backend-infra-nfs --type merge --patch '{"spec":{"managementLIF":"10.172.254.46"}}'

oc -n trident patch tbc backend-app-nfs --type merge --patch '{"spec":{"managementLIF":"10.172.254.47"}}'

oc get tbc -n trident

hsuma@plnx-admin:~$
hsuma@plnx-admin:~$ oc -n trident create secret generic backend-infra-secret --from-literal=username=vsadmin --from-literal=password='Infra$vm2026!Tr1dent' --dry-run=client -o yaml | oc apply -f -
secret/backend-infra-secret configured
hsuma@plnx-admin:~$
hsuma@plnx-admin:~$
hsuma@plnx-admin:~$ oc -n trident create secret generic backend-app-secret --from-literal=username=vsadmin --from-literal=password='App$vm2026!Tr1dent' --dry-run=client -o yaml | oc apply -f -
secret/backend-app-secret configured
hsuma@plnx-admin:~$ oc -n trident patch tbc backend-infra-nfs --type merge --patch '{"spec":{"managementLIF":"10.172.254.46"}}'
tridentbackendconfig.trident.netapp.io/backend-infra-nfs patched
hsuma@plnx-admin:~$ oc -n trident patch tbc backend-app-nfs --type merge --patch '{"spec":{"managementLIF":"10.172.254.47"}}'
tridentbackendconfig.trident.netapp.io/backend-app-nfs patched
hsuma@plnx-admin:~$ oc get tbc -n trident
NAME                BACKEND NAME        BACKEND UUID                           PHASE   STATUS
backend-app-nfs     backend-app-nfs     27f061fd-e6e4-448c-a4e4-c4302ac4a01a   Bound   Failed
backend-infra-nfs   backend-infra-nfs   88ba74b5-6d56-47cf-a16d-9066ae716681   Bound   Failed
hsuma@plnx-admin:~$ oc describe tbc -n trident backend-app-nfs
Name:         backend-app-nfs
Namespace:    trident
Labels:       <none>
Annotations:  <none>
API Version:  trident.netapp.io/v1
Kind:         TridentBackendConfig
Metadata:
  Creation Timestamp:  2026-02-17T21:31:39Z
  Finalizers:
    trident.netapp.io
  Generation:        2
  Resource Version:  917491
  UID:               df168257-e589-4246-903d-9d8a493252cc
Spec:
  Credentials:
    Name:    backend-app-secret
  Data LIF:  10.172.194.22
  Defaults:
    Export Policy:    default
    Snapshot Policy:  default
    Space Reserve:    none
  Management LIF:     10.172.254.47
  Storage:
    Defaults:
      Snapshot Policy:  default
      Space Reserve:    none
    Labels:
      Environment:  dev
    Defaults:
      Snapshot Policy:  default
      Space Reserve:    none
    Labels:
      Environment:  test
    Defaults:
      Snapshot Policy:  default
      Space Reserve:    volume
    Labels:
      Environment:      prod
  Storage Driver Name:  ontap-nas
  Svm:                  ocp_app_svm
  Version:              1
Status:
  Backend Info:
    Backend Name:         backend-app-nfs
    Backend UUID:         27f061fd-e6e4-448c-a4e4-c4302ac4a01a
  Deletion Policy:        delete
  Last Operation Status:  Failed
  Message:                Failed to apply the backend update; problem initializing storage driver 'ontap-nas': could not configure storage pools: could not get storage pools from array: SVM ocp_app_svm has no assigned aggregates
  Phase:                  Bound
Events:
  Type     Reason  Age                From                    Message
  ----     ------  ----               ----                    -------
  Warning  Failed  40s                trident-crd-controller  Failed to apply the backend update; problem initializing storage driver 'ontap-nas': error initializing ontap-nas driver; could not create Data ONTAP API client: error creating ONTAP API client: error reading SVM details: response code 401 (Unauthorized): incorrect or missing credentials
  Warning  Failed  4s (x24 over 36s)  trident-crd-controller  Failed to apply the backend update; problem initializing storage driver 'ontap-nas': could not configure storage pools: could not get storage pools from array: SVM ocp_app_svm has no assigned aggregates
hsuma@plnx-admin:~$



cat <<EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc-new
  namespace: default
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: netapp-app
EOF



vault secrets enable -path=pki_int pki
vault secrets tune -max-lease-ttl=43800h pki_int


vault write -format=json pki_int/intermediate/generate/internal \
  common_name="Calix OCP-AI Intermediate CA" \
  | jq -r '.data.csr' > /tmp/ocp-ai-intermediate.csr


time="2026-02-18T21:10:22Z" level=info msg="resolving subscriptions in namespace" id=Gs+ao namespace=openshift-gitops-operator
time="2026-02-18T21:10:22Z" level=info msg="unpacking bundles" id=Gs+ao namespace=openshift-gitops-operator
time="2026-02-18T21:10:22Z" level=info msg="unpacking is not complete yet, requeueing" id=Gs+ao namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="resolving sources" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="checking if subscriptions need update" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="checking for existing installplan" channel=latest id=9S1kY namespace=openshift-gitops-operator pkg=openshift-gitops-operator source=redhat-operators sub=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="resolving subscriptions in namespace" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="unpacking bundles" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:27Z" level=info msg="unpacking is not complete yet, requeueing" id=9S1kY namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="resolving sources" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="checking if subscriptions need update" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="checking for existing installplan" channel=latest id=B+keL namespace=openshift-gitops-operator pkg=openshift-gitops-operator source=redhat-operators sub=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="resolving subscriptions in namespace" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="unpacking bundles" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="bundle unpacking failed. Reason: DeadlineExceeded, and Message: Job was active longer than specified deadline" id=B+keL namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg=syncing reconciling="*v1alpha1.Subscription" selflink=
time="2026-02-18T21:10:32Z" level=info msg="evaluating current pod" correctHash=true correctImages=true current-pod.name=certified-operators-qf99z current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="of 1 pods matching label selector, 1 have the correct images and matching hash" correctHash=true correctImages=true current-pod.name=certified-operators-qf99z current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="evaluating current pod" correctHash=true correctImages=true current-pod.name=community-operators-2ccwz current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="of 1 pods matching label selector, 1 have the correct images and matching hash" correctHash=true correctImages=true current-pod.name=community-operators-2ccwz current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="evaluating current pod" correctHash=true correctImages=true current-pod.name=redhat-marketplace-xl6qt current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="of 1 pods matching label selector, 1 have the correct images and matching hash" correctHash=true correctImages=true current-pod.name=redhat-marketplace-xl6qt current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="evaluating current pod" correctHash=true correctImages=true current-pod.name=redhat-operators-qvxvw current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="of 1 pods matching label selector, 1 have the correct images and matching hash" correctHash=true correctImages=true current-pod.name=redhat-operators-qvxvw current-pod.namespace=openshift-marketplace
time="2026-02-18T21:10:32Z" level=info msg="resolving sources" id=YkE7T namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="checking if subscriptions need update" id=YkE7T namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="checking for existing installplan" channel=latest id=YkE7T namespace=openshift-gitops-operator pkg=openshift-gitops-operator source=redhat-operators sub=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="resolving subscriptions in namespace" id=YkE7T namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="unpacking bundles" id=YkE7T namespace=openshift-gitops-operator
time="2026-02-18T21:10:32Z" level=info msg="bundle unpacking failed. Reason: DeadlineExceeded, and Message: Job was active longer than specified deadline" id=YkE7T namespace=openshift-gitops-operator
hsuma@plnx-admin:~/ocp-ai-config$




oc patch catalogsource redhat-operators -n openshift-marketplace --type merge --patch '{"spec":{"grpcPodConfig":{"memoryTarget":"512Mi"}}}'


Hi Haj
Let’s chat about this.  I want to know if calix.local CA is the right PKI for this.
 
It’s interesting, Frank Ye had a similar request just last week.  This would require IT to allow us to be an issuer for the enterprise PKI (calix.local).  We’ll need to present a strong case for it.
 
I have a questions though.  So please grab some time, maybe Friday morning?  I’d like to understand if / why we want to use calix.local or if it would make more sense to use a self-signed CA.
 
One main question I will have; will any of these certs be used by “Customer” facing applications or is this purely for internal cluster communications?



Hey Brian,

To answer your main question upfront: these certs are purely for internal cluster communications. No customer-facing applications. Specifically, they'd cover the OpenShift API endpoint (api.ocp-ai.calix.local), the web console, and internal application routes (*.apps.ocp-ai.calix.local).

The reason we'd prefer to chain under the Calix CA rather than self-signed is that all internal machines already trust it — so anyone accessing the cluster won't get certificate warnings or need to manually import a new root cert.

We're not looking to become a general issuer for the enterprise PKI. The ask is a single intermediate CA with pathlen:0 (can't create sub-CAs), scoped to our OCP-AI cluster and managed through HashiCorp Vault with automated issuance and rotation.



Hey Sulabh, to follow up on your question about CNPG and vector databases — yes, CNPG supports vector databases through the pgvector extension on PostgreSQL. We can set that up on the new OCP-AI cluster.

We're building out the AI platform on-prem now and I want to make sure we match what your team needs. Can you share what your current stack looks like on GCP? Specifically:

- ML platform (Vertex AI, Kubeflow, etc.)
- Notebook environment
- Model serving framework
- Object storage
- Databases (relational + vector)

That way I can set up the same or similar services on OpenShift. Thanks!





Hey Sulabh,

To follow up on your question about CNPG and vector databases — yes, CNPG supports vector databases through the pgvector extension on PostgreSQL. We can set that up on the new OCP-AI cluster.

I also know you mentioned your team is using MinIO. We're planning to deploy MinIO on the cluster as well for S3-compatible object storage backed by NetApp.

We're building out the full AI platform on-prem now and I want to make sure we match what your team needs. Can you share what your current stack looks like on GCP? Specifically:

- ML platform (Vertex AI, Kubeflow, etc.)
- Notebook environment
- Model serving framework
- Databases (relational + vector)
- Any other services your workflows depend on

That way I can set up the same or similar services on OpenShift.

Thanks!
