Sorry for the delayed response - I was prioritizing some Ubuntu Landscape work and also wanted to take the time to go deeper into the draft.
Overall the scope looks strong - disconnected install, GPU enablement, NetApp integration, CI/CD path - all the right areas are covered.
 
A few points I’d like to see tightened up so the design package is directly actionable for us:
 
Hardware alignment: Phase 1 mentions UCS separation and GPU roles, but I’d like the design to explicitly map to our footprint: 3 UCS blades for masters/infra/CPU workers, Dell R760XA GPU servers, and NetApp A30 for storage. That way HA and sizing guidance aren’t left generic.
 
High availability: Helpful if the docs spell out what HA means with our setup - 3 masters for quorum, infra nodes with at least 2 replicas, app pods with PDBs/anti-affinity so draining a blade doesn’t cause disruption. For GPUs, HA is only achievable if we have 2+ nodes, which should be noted.
 
Environment: We’ll have dev, staging, and prod. We need to decide early whether dev/staging live together in a non-prod cluster or if all three are separate. That impacts sizing and upgrade workflows.
 
Security: The draft already covers RBAC, IdP integration, and network policy, which is good. I’d like to confirm whether policy enforcement (OPA/Gatekeeper or Kyverno) will be part of the design. That would give us guardrails like blocking privileged pods, enforcing trusted registries, and resource quotas. We’d also need an exemption label/annotation pattern so we can handle legitimate exceptions (GPU operators, infra agents, etc.) without weakening overall controls.
 
Security scanning - I don’t see image scanning or runtime security tools mentioned. Do we expect GDT to design around Red Hat Advanced Cluster Security (StackRox) or another scanner, or is that being handled separately
 
Secret management - Since we already use CyberArk on-prem, are we expecting GDT to integrate OpenShift with CyberArk for secrets, or fall back to OCP native secrets with etcd encryption? It would help to clarify the intended pattern for developers.
 
Identity integration -  The draft mentions enterprise IdP integration. Can we plan on leveraging our existing IPA/IdM for OpenShift OAuth and group-to-role mappings? That would let us reuse our current identity model and avoid duplicating access control.
 
Data services: I don’t see anything in scope around CNPG or other Postgres operators. Since we know the AI workloads will have a database dependency, we should clarify whether GDT is expected to design/deploy CNPG (or equivalent) for HA Postgres, or if that will be handled separately.
 
Backup and restore:  Phase 4 lists backup/restore in the runbook, but it’s not clear how deep this goes. We’ll need a defined backup strategy (Velero/OADP for cluster resources, etcd snapshots, PV backups, and integration with NetApp SnapMirror) plus restore testing. Without that, we don’t really have a recovery plan if something goes sideways.
 
Day-2 operations:  Phase 4 mentions OTA upgrades during a controlled change window, which is the right call. I’d like to make sure the deliverable includes runbook-level detail - drain/cordon sequencing across UCS blades, handling GPU workloads during patching (since they don’t survive eviction the same way), and rollback validation if an OTA fails. Also worth detailing UCS firmware upgrades alongside OpenShift OTAs.
 
On the handoff, it would be valuable for our team to get direct exposure to oc-mirror workflows, GPU Operator lifecycle, and Trident troubleshooting so we’re not dependent long-term.
 
A couple of clarifications for you:
Networking/ToR redundancy -> Does the design assume dual ToRs (or equivalent dual-fabric via Nexus FIs) for UCS/GPU nodes? With only three UCS blades in the pool, I want to make sure we’re not in a position where a single ToR failure or patch takes the cluster offline.
 
NetBox/IPAM -> Are we planning to leverage NetBox as the IPAM/DCIM source of truth for this deployment, or just manage IP allocations manually? I don’t see it mentioned in the draft.
 
If GDT can update the design package to reflect our actual hardware, document HA and environment strategy explicitly, clarify the approach for policy enforcement, scanning, secrets, and IdM, cover the CNPG/database operator gap, and expand the backup/restore and ops runbook detail, I think the scope will be in good shape to move forward with this guy
 
Thanks,
Haj



GPU Licensing:
For the H100 GPUs, NVIDIA AI Enterprise licenses are required (one per GPU) to unlock the supported drivers, vGPU, and NLS capabilities. The H100s are fully supported on this platform. In addition, Red Hat AI Accelerator (SKUs MCT4721/MCT4722) provides GPU enablement within OpenShift. Licensing is also one per GPU. Based on the BOM, it looks like there are eight production GPUs and possibly sixteen assigned for staging, which may indicate some overspend or duplication to review.




OpenShift Platform Readiness – Pre-Install Dependencies
Allocate Network CIDRs for OpenShift Platform & Workloads

Description
Allocate and document network CIDRs required for the OpenShift platform, including infrastructure nodes, application/workload networks, and ingress/load balancer VIPs.
This task focuses on address allocation only, not firewall or policy implementation.

Key Outcomes

CIDRs allocated for platform, workloads, and ingress

CIDRs documented and approved

No overlap with existing environments

Confirm Network Reachability for OpenShift Dependencies

Description
Confirm that the OpenShift platform CIDRs can reach required enterprise services at a high level.
This task documents expected network flows (no ports) to align with the network team ahead of installation.

Dependencies Include

API and ingress load balancers

NetApp storage

On-prem container registry

Identity provider (FreeIPA / IdM)

HashiCorp Vault

Centralized logging (Graylog)

Key Outcomes

High-level reachability confirmed

Expected network flows documented

Network team alignment achieved

Assess On-Prem Container Registry Using Existing Cluster

Description
Assess the existing on-prem container registry using the current AI-enabled Kubernetes (RKE2) cluster to ensure it can support a disconnected OpenShift environment.
This includes evaluating image push/pull workflows, authentication/TLS, and available image vulnerability scanning or supported integrations.

Key Outcomes

Image push and pull validated from RKE2

TLS and authentication confirmed

Image scanning capability or integration path identified

Registry suitability for disconnected OpenShift documented


HashiCorp Vault Integration Readiness (GitOps)

Description
Confirm HashiCorp Vault integration readiness using a GitOps-driven workflow on the existing on-prem AI-enabled Kubernetes (RKE2) cluster.
The goal is to validate a reusable secrets management pattern for OpenShift.

Key Outcomes

Kubernetes authentication to Vault validated

Secrets consumed by workloads without manual handling

Git-managed manifests reference Vault-backed secrets

Reusable integration pattern documented


Set Up and Exercise GitOps Tooling

Description
Set up an initial Git repository and exercise GitOps workflows using FluxCD on the existing cluster to confirm readiness for OpenShift.

Key Outcomes

Git repository created and accessible

FluxCD installed and healthy on RKE2

Git-driven changes successfully reconciled

GitOps pattern validated for reuse


Review Jenkins Readiness for OpenShift Workflows

Description
Review whether existing Jenkins systems can support OpenShift-related workflows (image builds, GitOps triggers, automation), or if a dedicated Jenkins instance is required.

Key Outcomes

Existing Jenkins capabilities assessed

Reuse vs dedicated Jenkins decision documented

Risks and follow-up actions identified


Check Identity Provider Readiness (FreeIPA / IdM)

Description
Check that the existing FreeIPA / IdM environment is ready to integrate with OpenShift for authentication and RBAC.
This will be validated by integrating FreeIPA / IdM with the existing on-prem AI-enabled Kubernetes (RKE2) cluster to confirm the authentication and group-mapping approach.

Key Outcomes

Authentication against IdM validated

Group-to-role mapping confirmed

Required certificates and trust relationships identified

Integration pattern documented for OpenShift


Confirm Centralized Logging Pattern for OpenShift

Description
Confirm that centralized logging is enabled and working on the current on-prem AI-enabled Kubernetes cluster, and determine whether the same logging approach can be reused for OpenShift.

Key Outcomes

Logs successfully forwarded from RKE2 workloads

Graylog connectivity and assumptions confirmed

Logging integration pattern documented for OpenShift
