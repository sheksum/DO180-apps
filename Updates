Task 1 — OpenShift Installation → Complete
OCP 4.20.13 installed via Red Hat Assisted Installer on bare metal. 3 UCS-X masters + 2 Dell GPU workers, all 5 nodes Ready. Discovery ISO generated (full image), nodes booted via vMedia (KVM for UCS-X, iDRAC for Dell), roles assigned, bootstrap completed. kubeadmin and oc CLI configured on plnx-admin (10.172.248.50). NTP configured via MachineConfigs (99-master-chrony, 99-worker-chrony) pointing to time1/time2.calix.local. First install attempt failed due to vMedia disconnect at 28% — second attempt succeeded.
	∙	Task 2 — Dell GPU Server Setup → Complete
2x Dell R760xa GPU workers joined cluster. VLAN 1510 configured on Leaf-3 switch (was missing, blocked initial join). Discovery ISO booted, hosts registered, hostnames set (gpu01/gpu02), roles assigned as Worker. CoreOS installed, worker CSRs approved. MTU mismatch resolved — cluster overlay expects 8958, workers came up at 1500, ovnkube-node failed with NetworkPluginNotReady. Fixed with MachineConfig 99-worker-mtu-9000 (systemd unit: ip link set eno8303 mtu 9000 && ovs-vsctl set int br-ex mtu_request=9000) plus jumbo frames enabled on switch. Per server: 4x H100L 94GB, 128 HT cores (Xeon 8562Y+), 2 TiB RAM, 480 GB NVMe boot, 26.88 TB RAID, Mellanox ConnectX-6 Dx 100GbE.
	∙	Task 3 — Storage Networking → Complete
NMState NNCPs created per master node with bond bnd0 (eno7 + eno8, active-backup mode, MTU 9000). 6 VLAN subinterfaces configured: 1792 (Dev Infra), 1794 (Dev App), 1796 (Test Infra), 1798 (Test App), 1793 (Prod Infra), 1797 (Prod App). IPs assigned following pattern — third octet matches VLAN subnet, host octet matches management IP (.150/.151/.152). Jumbo frames validated end-to-end to all NetApp LIFs. All NNCPs SuccessfullyConfigured. Bond named “bnd0” because Linux interface name max is 15 chars — “bond-storage.1792” would be 18 and NMState fails silently.
	∙	Task 4 — NetApp Trident CSI → Complete
Trident operator installed from OperatorHub (NetApp Trident Certified). Two backends created: backend-infra-nfs (ocp_infra_svm, data LIF 10.172.192.22, mgmt LIF 10.172.254.46) and backend-app-nfs (ocp_app_svm, data LIF 10.172.194.22, mgmt LIF 10.172.254.47). Two StorageClasses: netapp-infra (not default, Retain) and netapp-app (default, Retain). Both ontap-nas, NFS v4.1. PVC creation validated. Pod read/write validated. Originally deployed with cluster admin creds on cluster mgmt LIF — migrated to vsadmin creds per SVM using SVM mgmt LIFs (Feb 18). Aggregates aggr1_n1 through n6 assigned to both SVMs to resolve vsadmin visibility issue. Export policies (backend_infra_access, backend_app_access) include mgmt subnet 10.172.248.0/24 so GPU workers can mount NFS without storage VLANs.
	∙	Task 5 — Internal Image Registry → Complete
Registry enabled in Managed state with 2 replicas. Backed by 100Gi PVC on netapp-infra StorageClass. Image push/pull validated. Pinned to master nodes via nodeSelector — GPU workers don’t have storage VLANs yet and NFS mounts would hang. Default route enabled with 600s timeout. Registry constraint can be relaxed once GPU NIC migration completes and storage VLANs are configured on GPU workers.​​​​​​​​​​​​​​​​